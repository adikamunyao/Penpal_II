{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d7bc516",
   "metadata": {},
   "source": [
    "<center><h1>Handwritten to Digital.<h1/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5677e5",
   "metadata": {},
   "source": [
    "<h2>Abstract</h2>\n",
    "\n",
    "Handwritten notes are commonly used to write and share information. However, since they are written on paper, it is difficult to search for specific content, store them, and share them easily. By turning handwritten notes into digital files, we can enjoy many advantages, such as being able to search for information quickly, store them in a better way, and use advanced techniques to analyze the data they contain\n",
    "\n",
    "During this text recognition, we process the input image, extraction of features, and classification schema takes place, training of system to acknowledge the text. During this approach, the system is trained to seek out the similarities, and also the differences among various handwritten samples. This application takes the image of a hand transcription and converts it into a digital text\n",
    "\n",
    "**Keyswords**\n",
    "* HTR - (Handwritten Text Recognition)\n",
    "* NN - (NeuralNetwork)\n",
    "* CNN - (convolutional Neural Network)\n",
    "* RNN - n(RecurrentNeural Network)\n",
    "* CTC - (Connectionist Temporal Classification)\n",
    "* TF - (TensorFlow)\n",
    "* CPU - (Central Processing Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53805c9",
   "metadata": {},
   "source": [
    "<h3><center> I. INTRODUCTION</h3>\n",
    "\n",
    "Image processing could be a manipulation of images within the computer vision.There are many techniques for the manipulation of the images. The character recognition involves several steps like acquisition, feature extraction, classification, and recognition. Handwriting recognition is the ability of a machine to receive and interpret the handwritten input from an external source like image. The most aim of this project is to model a system that may efficiently recognize the actual character of format employing a neural network.\n",
    "\n",
    "A handwriting recognition system handles formatting, performs correct segmentation into characters, and finds the foremost plausible words. Off-line handwriting recognition involves the automated conversion of the text in a picture into letter codes that are usable within computer and text-processing applications. the info obtained by this manner is considered a static representation of handwriting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daf1023",
   "metadata": {},
   "source": [
    "<h4> Related Work</h4>\n",
    "\n",
    ">Various pre-processing techniques are used by K. Gaurav and Bhatia for character recognition. These techniques handle different types of images, including handwritten documents and documents with colored backgrounds. They also deal with varying intensities of images. The offline character recognition method proposed involves diagonal feature extraction and relies on an ANN model. Two approaches for creating a neural network system are discussed, one with 54 features and the other with 69 features.\n",
    "\n",
    ">A system for offline cursive handwriting recognition is described by A. Brakensiek, J. Rottland, A. Kosmala, and J. Rigoll. The system is based on Hidden Markov Models (HMM) and uses discrete and hybrid modeling techniques. R. Bajaj, L. Dey, and S. Chaudhari employ three different styles of features (density, moment, and descriptive component features) for classifying Devanagari Numerals, achieving an accuracy of 89.6% for handwritten Devanagari numerals.\n",
    "\n",
    ">M. Hanumadhulu and O.V. Ramanammurty implement a fuzzy set approach using the box method, achieving a recognition accuracy of 90%. This model works with various sources of information. Previous research has shown that this model performs well with diverse information sources, but it may be less accurate when dealing with long sentences. Many proposed models struggle to accurately classify long text data. On the other hand, models incorporating CNN networks are showing promising results due to their ability to handle longer text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fa3531",
   "metadata": {},
   "source": [
    "<h3><center>II. PROPOSED WORK</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ef4e77",
   "metadata": {},
   "source": [
    "<h4>A. Handwritten text recognition</h4>\n",
    "Handwritten Text Recognition (HTR) systems are used to recognize handwritten text from scanned images. In this case, a Neural Network (NN) will be constructed and trained on word-images from the IAM dataset. Since the input layer, along with all the other layers, can be kept small for word-images, training the NN can be done on the CPU, although using a GPU would be more efficient. TensorFlow (TF) is the minimum requirement for implementing HTR.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e153c3",
   "metadata": {},
   "source": [
    "<h4>B. Model Overview</h4>\n",
    "For the task at hand, a Neural Network (NN) is utilized, which comprises several types of layers. These layers include Convolutional Neural Network (CNN) layers, Recurrent Neural Network (RNN) layers, and a final layer known as Connectionist Temporal Classification (CTC) layer. The CNN layers are responsible for extracting features from the input images, while the RNN layers capture the sequential dependencies within the extracted features. The CTC layer is used for decoding the sequential predictions into the final recognized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da0d80f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./data/img.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary library\n",
    "from IPython.display import Image\n",
    "Image(url='./data/img.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976c26c0",
   "metadata": {},
   "source": [
    ">In this project, a configuration of 5 CNN layers for feature extraction, followed by 2 RNN layers and a CTC layer for loss calculation, is utilized. The initial step involves preprocessing the images to minimize noise interference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa289fa",
   "metadata": {},
   "source": [
    "The Neural Network (NN) can be represented in a more formal manner as a function, as shown in Equation below. This function maps an input image or matrix (M) with dimensions W×H to a character sequence (c1, c2, ...) with a length ranging from 0 to L. It is important to note that the text recognition process operates at the character level, allowing for the recognition of words or texts that are not present in the training data, as long as the individual characters are classified correctly.\n",
    "> ### NN: M (C1, C2, ………, Cn)\n",
    "\n",
    ">> ### W x H 0 ≤ n ≤ L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30625e9",
   "metadata": {},
   "source": [
    "<h4>C. Operation</h4>\n",
    "   <h5>CNN</h5>\n",
    "    In the CNN layers, the input image is passed through a series of operations to extract relevant features. Each layer follows three steps. First, a convolution operation is performed using a 5x5 filter in the first two layers and a 3x3 filter in the last three layers. Next, the non-linear ReLU function is applied to introduce non-linearity. Finally, a pooling layer summarizes image regions and produces a downsized version of the input. With each layer, the height of the image is reduced by a factor of 2, and additional feature channels are added. As a result, the output feature sequence has a size of 32x256.\n",
    "    \n",
    "   <h5>RNN</h5>\n",
    "    In the RNN component, the feature sequence consists of 256 features for each time-step. The RNN is responsible for propagating relevant information throughout this sequence. To achieve this, the preferred implementation of RNN used is Long Short-Term Memory (LSTM). LSTM is chosen because it can effectively propagate information over longer distances and offers more robust training characteristics compared to vanilla RNN.\n",
    "\n",
    "The output sequence generated by the RNN is then mapped to a matrix with dimensions of 32x80.\n",
    "\n",
    "Regarding the IAM dataset, it contains 79 different characters. In addition to these 79 characters, an extra character is required for the Connectionist Temporal Classification (CTC) operation, specifically the CTC blank label. Consequently, for each of the 32 time-steps, there are a total of 80 entries in the output matrix.\n",
    "    <h5>CTC</h5>\n",
    "    During the training of the Neural Network (NN), the Connectionist Temporal Classification (CTC) is provided with the RNN output matrix and the ground truth text. The CTC computes the loss value by comparing the predicted sequence with the actual (ground truth) text. This loss value is used to update the parameters of the NN during the training process.\n",
    "\n",
    "   During inference or prediction, the CTC is given only the RNN output matrix. It decodes the matrix to generate the final recognized text. It should be noted that both the ground truth text (used during training) and the recognized text (output during inference) can have a maximum length of 32 characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08a42da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./data/img2.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize proposed network\n",
    "Image(url='./data/img2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c1e919",
   "metadata": {},
   "source": [
    "<h3><center>III. Data</h3>\n",
    "The IAM Handwriting Database contains forms of handwritten English text which can be used to train and test handwritten text recognizers and to perform writer identification and verification experiments.\n",
    "\n",
    "* 657 writers contributed samples of their writings.\n",
    "* 1532 pages of scanned text.\n",
    "* 5685 isolated and labeled sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd435dd",
   "metadata": {},
   "source": [
    "<h5>NN INPUT</h5>\n",
    "    The input for the Neural Network (NN) is a gray-value image with dimensions of **128x32.** However, the images from the dataset may not have the exact same size. To address this, we resize the image, maintaining its aspect ratio, until it reaches either a width of 128 or a height of 32. Subsequently, we place the resized image onto a target image of size 128x32, which is white in color.\n",
    "\n",
    "Afterwards, the gray-values of the image are normalized to simplify the task for the NN. This normalization process adjusts the range of gray-values to a standardized scale. Additionally, data augmentation techniques can be incorporated by randomly copying the image to various positions instead of aligning it to the left, or by randomly resizing the image. These techniques can help diversify the dataset and enhance the robustness of the NN's training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef47d1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./data/ip.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize input data\n",
    "Image(url='./data/ip.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7782ec2c",
   "metadata": {},
   "source": [
    "<h5>CNN OUTPUT</h5>\n",
    "The output of the CNN layers, as shown above, is a sequence of length 32. Each entry in the sequence represents the features extracted by the CNN layers and contains 256 features. While these features are further processed by the RNN layers, it is worth noting that certain features from the CNN layers already exhibit a high correlation with specific high-level properties of the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3117d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./data/CNN1.png\" width=\"800\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Visualize top extracted features\n",
    "Image(url='./data/CNN1.png', width=800, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adabd5c2",
   "metadata": {},
   "source": [
    "    * Top: 256 features per time-step is computed by the CNN layers.\n",
    "    * Middle: input image.\n",
    "    * Bottom: plot of the 32nd feature, which incorporates a high correlation with the occurrence of the character “e” within the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf87a50",
   "metadata": {},
   "source": [
    ">Some features may show a high correlation with certain characters, such as \"e\", or with duplicated characters, such as \"ll\". Additionally, there may be features that capture properties of characters, such as loops commonly found in handwritten \"l\" and \"e\". These observations suggest that the CNN layers are capable of identifying and extracting meaningful information related to character-level properties from the input image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809e3451",
   "metadata": {},
   "source": [
    "<h5>RNN OUTPUT</h5>\n",
    "The output of the RNN is illustrated below, representing a matrix that visualizes the scores associated with the characters, including the special blank label used in Connectionist Temporal Classification. The matrix is organized vertically, with the top-most graph displaying the scores for each character, followed by the characters \"!\",\"#&'()*+,-./ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789:;?\". The alignment of the characters is accurate except for the last character \"e,\" but this discrepancy is inconsequential because the CTC operation disregards absolute positions and is segmentation-free.\n",
    "\n",
    "In the bottom-most graph, the scores for the characters \"l,\" \"i,\" \"t,\" \"e,\" and the CTC blank label are shown. Decoding the text from these scores is straightforward: the most probable character is selected from each time-step, creating a sequence known as the best path. Repeated characters are eliminated, and all blanks are removed. Following this process, the decoded text is \"l---i--t-t--l-...-e,\" which can be further simplified to \"l---i--t-t--l-...-e\" and finally \"little.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac6dfac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./data/RNNop.png\" width=\"800\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize RNN output\n",
    "Image(url='./data/RNNop.png', width=800, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacd2879",
   "metadata": {},
   "source": [
    "    * Top: output matrix of the RNN layers.\n",
    "    * Middle: input image.\n",
    "    * Bottom: Probabilities for the characters “l”, “i”, “t”, “e” and therefore the CTC blank label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c19b41",
   "metadata": {},
   "source": [
    "<h3><center>IV. Implementation using TF</h3>\n",
    "    The implementation is divided into four modules:\n",
    "\n",
    "1. SamplePreprocessor.py: This module is responsible for preparing the pictures from the IAM dataset for the neural network. It performs tasks such as reading the images, resizing them, and normalizing the pixel values.\n",
    "\n",
    "2. DataLoader.py: The DataLoader module reads the prepared samples and organizes them into batches. It also provides an iterator interface to traverse through the dataset, making it easier to feed the data to the model during training.\n",
    "\n",
    "3. Model.py: This module is responsible for creating the neural network model described in the implementation. It handles loading and saving models, manages TensorFlow sessions, and provides an interface for training the model and making predictions (inference).\n",
    "\n",
    "4. main.py: The main module brings together all the previously mentioned modules. It acts as the entry point of the implementation and coordinates the interactions between the SamplePreprocessor, DataLoader, and Model modules. It loads the data, initializes the model, and performs training or inference based on the requirements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895610e2",
   "metadata": {},
   "source": [
    "#### CNN:\n",
    "To construct each CNN layer, generate a **kernel** with a size of **k by k** for the convolution operation. Apply the ReLU activation function to the **pooling** layer with a size of **px by py** and a stride of sx×sy using the results from the convolution. Repeat these steps for all layers within a for-loop.\n",
    "\n",
    "#### RNN:\n",
    "Build two RNN layers, each consisting of 256 units, and stack them together. Then, create a bidirectional RNN by traversing the input sequence both from the beginning to the end and from the end to the beginning. This produces two output sequences, forward and backward, with dimensions of 32×256 each. Concatenate these sequences along the feature-axis to create a sequence of size 32×512. Finally, map this sequence to the output sequence (or matrix) of size 32×80, which is then fed into the CTC (Connectionist Temporal Classification) layer.\n",
    "\n",
    "#### CTC:\n",
    "For loss calculation, both the ground truth text and the matrix are fed into the operation. The ground truth text is encoded as a sparse tensor. The length of the input sequences needs to be provided for both CTC operations.\n",
    "\n",
    "#### Training:\n",
    "The mean of the loss values of the batch elements is employed to coach the NN: it's fed into an optimizer like\n",
    "RMSProp.\n",
    "\n",
    "#### Improving the model:\n",
    "* To feed complete text-lines instead of word-images, you have to increase the input size of the NN.\n",
    "* To improve the recognition accuracy, you can follow one of these hints:\n",
    ">* Data augmentation: increase dataset-size by applying further (random) transformations to the input images\n",
    ">* Remove cursive writing style in the input images\n",
    ">* Increase input size (if an input of NN is large enough,complete text-lines can be used)\n",
    "\n",
    "#### Spell checker\n",
    "Checking of spelling may be a basic requirement in any text processing or analysis. The python package pyspellchecker provides us this feature to search out the words that will are misspelled and also suggest the possible corrections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c069a45",
   "metadata": {},
   "source": [
    "<h3><center>V. The Architecture Of The Proposed Network</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2820377f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./data/arch.png\" width=\"500\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize archetecture\n",
    "Image(url='./data/arch.png', width=500, height=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e2b873",
   "metadata": {},
   "source": [
    ">Architecture of proposed network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3f1065",
   "metadata": {},
   "source": [
    "#### <center>CNN layers\n",
    "\n",
    "CNN is designed to mimic human visual processing and is specifically optimized for processing 2D images. It excels at learning and extracting 2D features efficiently. The max-pooling layer in CNN is particularly effective at capturing shape variations. Additionally, CNN utilizes tied weights, resulting in fewer parameters compared to a fully connected network of similar size. One of the key advantages of CNN is its ability to be trained using gradient-based learning algorithms, which mitigates the issue of vanishing gradients. By training the entire network to minimize an error criterion directly, CNN can generate highly accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c7a6adc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./data/cnnlayers.png\" width=\"800\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize CNN\n",
    "Image(url='./data/cnnlayers.png', width=800, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba27d11b",
   "metadata": {},
   "source": [
    "#### <center>RNN layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
